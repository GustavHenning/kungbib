

%%
%% forked from https://gits-15.sys.kth.se/giampi/kthlatex kthlatex-0.2rc4 on 2020-02-13
%% expanded upon by Gerald Q. Maguire Jr.
%% This template has been adapted by Anders Sjögren to the University
%% Engineering Program in Computer Science at KTH ICT. Adaptation is the
%% translation of English headings into Swedish as the addition of Swedish
%% text. Original body text is deliberately left in English.

%% Conventions for todo notes:
% \todo[inline]{Comments/directions/... in English}
% \todo[inline, backgroundcolor=kth-lightblue]{Text på svenska}
% \todo[inline, backgroundcolor=kth-lightgreen]{English descriptions about formatting}

%% The template is designed to handle a thesis in English or Swedish
% set the default language to english or swedish by passing an option to the documentclass - this handles the inside tile page
% To optimize for digital output (this changes the color palette add the option: digitaloutput
% To use bibtex or biblatex - include one of these as an option
\documentclass[english, bibtex]{kththesis}
%\documentclass[swedish, biblatex]{kththesis}

% \usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\ifbiblatex
    %\usepackage[language=english,bibstyle=authoryear,citestyle=authoryear, maxbibnames=99]{biblatex}
     \usepackage[bibstyle=authoryear,citestyle=authoryear, maxbibnames=99,language=english]{biblatex}
    \addbibresource{references.bib}
\else
    % The line(s) below are for BibTeX
    \bibliographystyle{bibstyle/myIEEEtran}
    %\bibliographystyle{apalike}
\fi


% include a variety of packages that are useful
\include{lib/includes}
\include{lib/kthcolors}

%\glsdisablehyper
%\makeglossaries
%\makenoidxglossaries
%\include{lib/acronyms}                %load the acronyms file

\include{lib/defines}  % load some additional definitions to make writing more consistent

% The following is needed in conjunction with generating the DiVA data with abstracts and keywords using the scontents package and a modified listings environment
%\usepackage{listings}   %  already included
\ExplSyntaxOn
\newcommand\typestoredx[2]{\expandafter\__scontents_typestored_internal:nn\expandafter{#1} {#2}}
\ExplSyntaxOff
\makeatletter
\let\verbatimsc\@undefined
\let\endverbatimsc\@undefined
\lst@AddToHook{Init}{\hyphenpenalty=50\relax}
\makeatother


\lstnewenvironment{verbatimsc}
    {
    \lstset{%
        basicstyle=\ttfamily\tiny,
        backgroundcolor=\color{white},
        %basicstyle=\tiny,
        %columns=fullflexible,
        columns=[l]fixed,
        language=[LaTeX]TeX,
        %numbers=left,
        %numberstyle=\tiny\color{gray},
        keywordstyle=\color{red},
        breaklines=true,                 % sets automatic line breaking
        breakatwhitespace=true,          % sets if automatic breaks should only happen at whitespace
        %keepspaces=false,
        breakindent=0em,
        %fancyvrb=true,
        frame=none,                     % turn off any box
        postbreak={}                    % turn off any hook arrow for continuation lines
    }
}{}



%% definition of new command for bytefield package
\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}

%% Acronyms
% note that nonumberlist - removes the cross references to the pages where the acronym appears
% note that nomain - does not produce a main glossary, this only acronyms will be in the glossary
% note that nopostdot - will present there being a period at the end of each entry
\usepackage[acronym, section=section, nonumberlist, nomain, nopostdot]{glossaries}
\usepackage[automake]{glossaries-extra}
\ifinswedish
    %\usepackage{glossaries-swedish}
\fi

% Because backref is not compatible with biblatex
\ifbiblatex
    \usepackage[plainpages=false]{hyperref}
\else
    \usepackage[
    backref=page,
    pagebackref=false,
    plainpages=false,
                            % PDF related options
    unicode=true,           % Unicode encoded PDF strings
    bookmarks=true,         % generate bookmarks in PDF files
    bookmarksopen=false,    % Do not automatically open the bookmarks in the PDF reading program
    pdfpagemode=UseNone,    % None, UseOutlines, UseThumbs, or FullScreen
    ]{hyperref}
    \usepackage{backref}
    %
    % Customize list of backreferences.
    % From https://tex.stackexchange.com/a/183735/1340
    \renewcommand*{\backref}[1]{}
    \renewcommand*{\backrefalt}[4]{%
    \ifcase #1%
          \or [Page~#2.]%
          \else [Pages~#2.]%
    \fi%
    }
\fi
\usepackage[all]{hypcap}	%% prevents an issue related to hyperref and caption linking


\include{lib/includes-after-hyperref}

%\glsdisablehyper
\makeglossaries
\include{lib/acronyms}                %load the acronyms file

%% Information for inside title page
\title{News article segmentation using multimodal input}
\subtitle{Subtitle TODO}

% give the alternative title - i.e., if the thesis is in English, then give a Swedish title
\alttitle{Artikelsegmentering med multimodala artificiella neuronnätverk}
\altsubtitle{Undertitel på svenska}
% alternative, if the thesis is in Swedish, then give an English title
%\alttitle{This is the English translation of the title}
%\altsubtitle{This is the English translation of the subtitle}

\authorsLastname{Student}
\authorsFirstname{Gustav H.}
\email{ghenning@kth.se}
\kthid{u1337 TODO}
% If the student has an ORCiD - add it here
\orcid{0000-0002-00001-1234}
\authorsSchool{\schoolAcronym{CDATE}}

\supervisorAsLastname{Supervisor}
\supervisorAsFirstname{J. Nyberg}
\supervisorAsEmail{jaknyb@kth.se}
% If the supervisor is from within KTH add their KTHID, School and Department info
\supervisorAsKTHID{u1337 TODO}
\supervisorAsSchool{\schoolAcronym{EECS}}
\supervisorAsDepartment{Computer Science}

\examinersLastname{Aristides}
\examinersFirstname{Gionis}
\examinersEmail{argioni@kth.se}
% If the examiner is from within KTH add their KTHID, School and Department info
\examinersKTHID{uTODO}
\examinersSchool{\schoolAcronym{EECS}}
\examinersDepartment{Theoretical Computer Science}
% other for a examiner outside of KTH add their organization info
%\examinersOrganization{Timbuktu University, Department of Pseudoscience}


\hostorganization{National Library of Sweden, Kungliga Biblioteket}   % if there was a host organization


\date{\today}


% For a CDATE student the following are likely values:
\programcode{CDATE}
\courseCycle{2}
\courseCode{DA231X}
\courseCredits{30.0}
\examName{Degree of Master of Science in Engineering}
\subjectArea{Computer Science and Engineering}


%%%%% For the oral presentation
%% Add this information once your examiner has scheduled your oral presentation
\presentationDateAndTimeISO{2021-03-15 13:00}
\presentationLanguage{eng}
\presentationRoom{via Zoom https://kth-se.zoom.us/j/ddddddddddd}
\presentationAddress{Isafjordsgatan 22 (Kistagången 16)}
\presentationCity{Stockholm}

% When there are multiple opponents, separate their names with '\&'
% Opponent's information
\opponentsNames{A. B. Normal \& A. X. E. Normalè}

%%%%% for DiVA's National Subject Category information
%%% Enter one or more 3 or 5 digit codes
%%% See https://www.scb.se/contentassets/3a12f556522d4bdc887c4838a37c7ec7/standard-for-svensk-indelning--av-forskningsamnen-2011-uppdaterad-aug-2016.pdf
%%% See https://www.scb.se/contentassets/10054f2ef27c437884e8cde0d38b9cc4/oversattningsnyckel-forskningsamnen.pdf
%%%%
%%%% Some examples of these codes are shown below:
% 102 Data- och informationsvetenskap (Datateknik)    Computer and Information Sciences
% 10201 Datavetenskap (datalogi) Computer Sciences 
% 10202 Systemvetenskap, informationssystem och informatik (samhällsvetenskaplig inriktning under 50804)
% Information Systems (Social aspects to be 50804)
% 10203 Bioinformatik (beräkningsbiologi) (tillämpningar under 10610)
% Bioinformatics (Computational Biology) (applications to be 10610)
% 10204 Människa-datorinteraktion (interaktionsdesign) (Samhällsvetenskapliga aspekter under 50803) Human Computer Interaction (Social aspects to be 50803)
% 10205 Programvaruteknik Software Engineering
% 10206 Datorteknik Computer Engineering
% 10207 Datorseende och robotik (autonoma system) Computer Vision and Robotics (Autonomous Systems)
% 10208 Språkteknologi (språkvetenskaplig databehandling) Language Technology (Computational Linguistics)
% 10209 Medieteknik Media and Communication Technology
% 10299 Annan data- och informationsvetenskap Other Computer and Information Science
%%%
% 202 Elektroteknik och elektronik Electrical Engineering, Electronic Engineering, Information Engineering
% 20201 Robotteknik och automation Robotics
% 20202 Reglerteknik Control Engineering
% 20203 Kommunikationssystem Communication Systems
% 20204 Telekommunikation Telecommunications
% 20205 Signalbehandling Signal Processing
% 20206 Datorsystem Computer Systems
% 20207 Inbäddad systemteknik Embedded Systems
% 20299 Annan elektroteknik och elektronik Other Electrical Engineering, Electronic Engineering, Information Engineering
%% Example for a thesis in Computer Science and Computer Systems
\nationalsubjectcategories{10201, 10207}

% Enter the English and Swedish keywords here for use in the PDF meta data _and_ for later use
% following the respective abstract.
% Try to put the words in the same order in both languages to facilitate matching. For example:
\EnglishKeywords{Historical newspapers, Image segmentation, Multimodal learning, Deep learning, Digital humanities, Mask R-CNN}
\SwedishKeywords{Historiska tidningar, Bildsegmentering, Multimodal inlärning, Djupinlärning, Digital humaniora, Mask R-CNN}

% Put the title, author, and keyword information into the PDF meta information TODO
\include{lib/pdf_related_includes}


% the custom colors and the commands are defined in defines.tex    
\hypersetup{
	colorlinks  = true,
	breaklinks  = true,
	linkcolor   = \linkscolor,
	urlcolor    = \urlscolor,
	citecolor   = \refscolor,
	anchorcolor = black
}


\begin{document}
%\selectlanguage{swedish}
%
\selectlanguage{english}

%%% Set the numbering for the title page to a numbering series not in the preface or body
\pagenumbering{alph}
\titlepage
% document/book information page
\bookinfopage

% Frontmatter includes the abstracts and table-of-contents
\frontmatter
\setcounter{page}{1}
\begin{abstract}
% The first abstract should be in the language of the thesis.
% Abstract fungerar på svenska också.
  \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
eng
\end{scontents}
%%% The contents of the abstract (between the begin and end of scontents) will be saved in LaTeX format
%%% and output on the page(s) at the end of the thesis with information for DiVA facilitating the correct
%%% entry of the meta data for your thesis.
%%% These page(s) will be removed before the thesis is inserted into DiVA.
\begin{scontents}[store-env=abstracts,print-env=true]

In this century and the last, serious efforts have been made to digitize the contents housed by memory institutions across the world. In order to open up these volumes to content-based information retrieval, these digitized print media pages have to be segmented into semantically connected components, i.e. articles. To query on facets such as author, section, content type or other metadata, further processing of these documents is required to attain this information.  
Even though humans have shown exceptional ability to segment different types of elements into related components, even in languages foreign to them, this task has proven difficult for computers. The challenge of semantic segmentation in newspapers lies in the diversity of the medium: Newspapers have vastly different layouts, covering diverse content, from news articles to ads to weather reports.
The extraordinary progress made in the field of instance segmentation of real-world objects in the field of deep learning begs the question: \textit{Can the same methodology be applied in the domain of newspaper articles?}
The state-of-the-art approaches are made to detect real-world objects. In this domain we have access to textual information through a potentially noisy Optical Character Recognition. We investigate one possible approach to encode the textual signal into the image in an attempt to improve performance. 
Based on the newspaper from the National Library of Sweden, we investigate the predictive power of visual and textual features and their capacity to generalize across different typographic designs. Results show…


\end{scontents}


%Even LaTeX comments can be handled, for example: \% comment at end

\subsection*{Keywords}
\begin{scontents}[store-env=keywords,print-env=true]
% If you set the EnglishKeywords earlier, you can retrieve them with:
\InsertKeywords{english}
% If you did not set the EnglishKeywords earlier then simply enter the keywords here:
% comma separate keywords, such as: Canvas Learning Management System, Docker containers, Performance tuning

\end{scontents}
\end{abstract}
\cleardoublepage
\babelpolyLangStart{swedish}
\begin{abstract}
    \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
Historiska tidningar, Bildsegmentering, Multimodal inlärning, Djupinlärning, Digital humaniora, Mask R-CNN
\end{scontents}
\begin{scontents}[store-env=abstracts,print-env=true]

I detta och det förra århundradet har kraftiga åtaganden gjorts för att digitalisera traditionellt medieinnehåll som tidigare endast tryckts i pappersformat. För att kunna stödja sökningar och fasetter i detta innehåll krävs bearbetning på semantisk nivå, det vill säga att innehållet styckas upp på artikelnivå, istället för per sida. Trots att människor har lätt att dela upp innehåll på semantisk nivå, även på ett främmande språk, fortsätter arbetet för automatisering av denna uppgift. Utmaningen i att segmentera nyhetsartiklar återfinns i den otroliga mångfald av utseende och format. Innehållet är även detta väldigt mångfaldigt, där man återfinner allt ifrån faktamässiga artiklar, till debatter, listor av fakta och upplysningar, reklam och väder bland annat. Otroliga framsteg har gjorts inom djupinlärning just för objektdetektering och semantisk segmentering bara de senaste årtiondet. Frågan vi ställer oss är: \textit{Kan samma metodik appliceras inom domänen nyhetsartiklar?} Dessa modeller är skapta för att klassificera världsliga ting. I denna domän har vi tillgång till texten och dess koordinater via en potentiellt bristfällig optisk teckenigenkänning. Vi undersöker ett sätt att utnyttja denna textinformation i ett försök att förbättra resultatet i denna specifika domän. Baserat på data från Kungliga Biblioteket undersöker vi hur väl denna metod lämpar sig för uppstyckandet av innehåll i tidningar längsmed tidsperioder där designen förändrar sig markant. Resultaten visar....

\end{scontents}
\subsection*{Nyckelord}
\begin{scontents}[store-env=keywords,print-env=true]
% SwedishKeywords were set earlier, hence we can use alternative 2
\InsertKeywords{swedish}
%\todo[inline, backgroundcolor=kth-lightblue]{Nyckelord som beskriver innehållet i uppsatsrapporten}
\end{scontents}
\end{abstract}
\babelpolyLangStop{swedish}

\cleardoublepage

\section*{Acknowledgments }
\markboth{Acknowledgments}{}

In this section, I would like to acknowledge the people who have encouraged, influenced, helped and supported me during the duration of this study.
    I would like to start by thanking and acknowledging my examiner Aristides Gionis and my supervisor Jakob Nyberg. Text about their support. I would also like to thank Kungliga Biblioteket, especially Faton Rekathati and Chris Haffenden, for their encouragement, interest in the topic, and invaluable help across the timeline of the project. 
    
\acknowlegmentssignature

\fancypagestyle{plain}{}
\renewcommand{\chaptermark}[1]{ \markboth{#1}{}} 
\tableofcontents
  \markboth{\contentsname}{}

\cleardoublepage
\listoffigures

\cleardoublepage

\listoftables
\cleardoublepage
%\lstlistoflistings\todo[inline, backgroundcolor=kth-lightgreen]{If you have listings in your thesis. If not, then remove this preface page.}
%\cleardoublepage
% Align the text expansion of the glossary entries
\newglossarystyle{mylong}{%
  \setglossarystyle{long}%
  \renewenvironment{theglossary}%
     {\begin{longtable}[l]{@{}p{\dimexpr 2cm-\tabcolsep}p{0.8\hsize}}}% <-- change the value here
     {\end{longtable}}%
 }
\glsaddall
%\printglossaries[type=\acronymtype, title={List of acronyms}]
%\printglossary[style=mylong, type=\acronymtype, title={List of acronyms and abbreviations}]
\printglossary[type=\acronymtype, title={List of acronyms and abbreviations}]
%\todo[inline, backgroundcolor=kth-lightgreen]{The list of acronyms and abbreviations should be in alphabetical order based on the spelling of the acronym or abbreviation.
%}
%% The following label is essential to know the page number of the last page of the preface
%% It is used to computer the data for the "For DIVA" pages
\label{pg:lastPageofPreface}
% Mainmatter is where the actual contents of the thesis goes
\mainmatter
\glsresetall
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\selectlanguage{english}
\chapter{Introduction}
\label{ch:introduction}

Nowadays it is commonplace for many memory institutions to create and maintain large digital repositories that offer rapid, time- and location-independent access to documents. This is the continuous culmination of a digitization effort spanning several decades of a great achievement in terms of preservation and accessibility. The real promise of digitization however, is the exploitation of the contents of these digital assets\cite{jdmdh:7097}. As we will see in this chapter and the next, there are many challenges yet to be addressed before exploitation will be seamless and fully automatic. 

\section{Background and problem context}
\label{sec:background}
%\todo[inline, backgroundcolor=kth-lightblue]{svensk: Bakgrund}

The processing of historical newspapers consists of three steps: 
\begin{enumerate}
\item Facsimile processing: Deriving the structure and the text from the document images via Document layout analysis, commonly using Optical Character Recognition (\gls{OCR})
\item Content enrichment: Extracting and connecting relevant information from both the textual and visual parts of the contents)
\item Exploration support: To enable searching and visualizing the contents.  
\end{enumerate}

Document layout analysis aims at segmenting a document image into meaningful segments and at classifying those segments according to their contents. \cite{ESKENAZI20171} 
    
Within the domain of newspapers, the task of image segmentation and classification is particularly difficult due to the complexity and diversity of content across several typographic designs. Layout analysis is however essential for historical newspaper exploitation. From an information retrieval perspective, being able to query articles instead of whole pages, and being able to facet over different types of segments has undeniable advantages to all concerned end users. 

%\todo[inline]{Present the background for the area. Set the context for your project – so that your reader can understand both your project and this thesis. (Give detailed background information in Chapter 2 - together with related work.)
%Sometimes it is useful to insert a system diagram here so that the reader
%knows what are the different elements and their relationship to each
%other. This also introduces the names/terms/… that you are going to use
%throughout your thesis (be consistent). This figure will also help you later
%delimit what you are going to do and what others have done or will do.}

%As one can find in RFC 1235\,\cite{ioannidis_coherent_1991} multicast is useful for xxxx. A number of different \glspl{OS} have been used in this work, such as the following \glspl{OS}: UNIX, Linux, Windows, etc. The main focus will be on one \gls{OS}, namely Linux.

\section{Problem}
\label{sec:problem}

The first part of this project is concerned with the creation and definition of a dataset for news article segmentation. In the second part, a state-of-the-art neural network for object detection and segmentation in images is introduced. We investigate one method of adapting the architecture to the domain of newspaper segmentation. The problem statement can be formulated as follows:
	
	\textit{How well can neural networks, developed for object detection and segmentation of real-world objects, perform in the domain of news article segmentation?}

\section{Purpose}

The purpose of this thesis is two-fold. Firstly, we want to investigate the efficacy of the current state-of-the-art object detection and segmentation model in the domain of newspaper article segmentation. Second, we evaluate one method of including the textual modality in the algorithm. 

\section{Goals}

The goal of this project is to classify news article boundaries in digitized newspaper pages. This is done by investigating to which extent the classification can be improved by combining inputs from the image and text modalities, as opposed to the traditional unimodal image classification architectures. Specifically for the data provided by The National Library of Sweden, we further investigate the difficulty of classifying news article boundaries depending on the age of publication and hence the design changes taking place across this timeline. The research questions are:

\begin{enumerate}
\item Do multimodal neural network architectures outperform unimodal neural network architectures, using image and text input as opposed to only using image input, in the field of instance segmentation?
\item Is there a difference in how well multimodal neural network architectures perform in periods of changing typographic design as opposed to unimodal neural network architectures?
\end{enumerate}

\section{Research Methodology}

The initial phase of the project aims to investigate the efficiency of applying a state-of-the-art neural network developed for real world object detection and segmentation in the domain of newspaper article segmentation. The selected methods based on previous work from the literature study are then evaluated on the dataset created during this project. Alterations to the selected methods are then presented and evaluated. Finally the results are presented and later discussed.

\section{Delimitations}

We recognize that there are many ways to extend or alter a neural network to adapt to new input. Only one such method of approaching text modality is introduced and evaluated. Other methods will be discussed and suggested in the final chapter. Another delimitation is that the size of the datasets created in this project may be insufficient to show the full potential of this approach. Investigation into this question will be addressed in this thesis. Only a limited number of typographic designs will be evaluated in this project, which may impact the generality of the solution discussed.

\section{Ethical considerations}

The dataset created in this project consists of published newspapers which have seen wide circulation. No special consideration needs to be taken in terms of sensitive or personally identifiable information.   

\section{Structure of the thesis}

In chapter \ref{ch:background}, we introduce the relevant background to understand the subject matter and briefly recaps the previous work. In order to understand the solution used in this thesis, the concepts used will be introduced in chapter \ref{ch:preliminaries}. Chapter \ref{ch:data} describes in detail the dataset created from the data accessed at The National Library of Sweden.  Next, chapter \ref{ch:modelarch} introduces the neural network employed in the study, the relevant adaptations to the pre-processing and manipulation of the data, implementation details of the neural network architecture. Finally the metrics used to quantitatively describe the results and the results are presented in chapter \ref{ch:eval} and discussed in chapter \ref{ch:disc}, while outlining potential avenues for future work.

\cleardoublepage
\chapter{Background}
\label{ch:background}

In this chapter, the history of digitization of historical documents will be reviewed. Then, we will look at the history of segmentation algorithms, as well as the history of the deep learning model that we intend to extend and evaluate. Further details about how the deep learning model functions will be introduced in chapter \ref{ch:preliminaries}. 

\section{Digitization of historical documents}

Libraries and other memory institutions across the world house a large collection of diverse media, often printed. Serious efforts have been made in the past decades to digitize this content by scanning printed pages into images. Beyond this large feat in preservation and accessibility, these scanned images need further processing before becoming searchable to the end library user. 

In order to open up these volumes to content-based information retrieval, these digitized print media pages have to be segmented into semantically connected components, i.e. articles. Since even a single page can contain a variety of different types of content, retrieval based on articles (instead of whole pages) allow for more complex types of queries that are related to a single artifact. To be able to query on facets such as author, section, content type or other metadata, further processing of these documents is required to attain this information.  

Even though humans have shown exceptional ability to segment different types of elements into related components, even in languages foreign to them\cite{8270006}, this task has proven difficult for computers. 

The challenge of semantic segmentation in newspapers lies in the diversity of the medium: Newspapers have vastly different layouts, covering diverse content, from news articles to ads to weather reports. Layout and style differs across newspapers, and even within the same newspaper as stylistic trends change across time. 


\subsection{Digitization at the National Library of Sweden}

The National Library of Sweden (Kungliga Biblioteket) started digitizing their materials in the late 1990s. The motivation for digitization has shifted from only preservation to also improving access and facilitating research conducted on the content itself. \cite{Rekathati_2020} The digital archives of the National Library of Sweden contain scanned images from four of the country’s largest newspaper publications dating back to the 19th century. The digitization of these newspaper images has occurred in two steps: Firstly, a segmentation algorithm splits the newspaper page into larger layout component zones. Within each zone, further segmentation is performed to identify sub-zones. These sub-zones may either be composed of images or blocks of text. In the second step, OCR is applied to the segmented sub-zones to extract the textual contents. 

\section{Segmentation Algorithms}

In the survey Eskenazi et al\cite{ESKENAZI20171} a topology for organizing segmentation algorithms is introduced based on the details of the algorithmic approach. 

According to this survey, three groups of approaches can be identified: 

\begin{enumerate}
\item Top-down
\item Bottom-up
\item Hybrid algorithms
\end{enumerate}

We will first review the classical algorithms, which are typically constrained by either the layout of the document(Such as the Manhattan layout) or parameters of said document(Such as font size or line spacing), and finally review the previous works in the domain of deep learning, which belong to the hybrid algorithms.

Each algorithm noted in the following chapter is classified depending on from which perspective it starts it's processing. Top-down algorithms start from the whole page, partitioning into smaller and smaller parts. On the contrary, bottom-up algorithms try to agglomerate elements into bigger and bigger elements up towards the whole page. See figure \ref{fig:classicalgs} for a complete picture.

\subsection{Classical Approaches}

\textbf{Layout constrained algorithms}. The first algorithms to appear in semantic segmentation tasks usually make strong assumptions about the layout of the documents. The algorithms can be further categorized into three groups based on how they assume the layout as can be seen in Figure \ref{fig:classicalgs}.

The earliest attempts at semantic segmentation made clear assumptions about the document layout, either with grammar (a set of rules), or by assuming the Manhattan layout and using projection profiles.
    
Second came the algorithms that rely on Run-Length Smoothing Algorithm (RLSA), mathematical morphology or other filters. The characteristics of these filters reflect the assumptions made on the layout geometry. 
    
Lastly, algorithms focused on detecting straight lines or square borders appeared e.g. using the Hough transform. This includes detecting white space alignment in which case the lines may appear “invisible”.
    
\textbf{Parameter constrained algorithms}. Moving away from the rigid assumptions, the second group of algorithms try to adapt to local variations in the document in order to be able to segment a broader range of layouts without changing the algorithm itself. The drawback of these techniques is the increased complexity and number of parameters associated with the algorithms. These parameters can be difficult to tune and may require larger datasets to train on. In this group we find the clustering algorithms, the algorithms based on function analysis, and the classification algorithms. 

Finally, in an attempt to overcome the limitations of the already mentioned algorithms, several techniques are combined in hybrid algorithms. Thus they cannot be categorized as either bottom-up or top-down algorithms since they may be both.  

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/classicalalgs.png}
  \end{center}
  \caption{Hierarchical typology of document segmentation algorithms. Top-down (TD) and bottom-up (BU) algorithms are also denoted. Credit\cite{Rekathati_2020} }
  \label{fig:classicalgs}
\end{figure}

\subsection{Deep Learning Approaches}

In an attempt to trade prior handcrafted features for learning capabilities, machine learning techniques, especially deep neural networks have outperformed classical algorithms in recent papers. These approaches can be categorized by the input they receive.

\textbf{Image modality only}. Convolutional neural networks were originally created to classify objects in images such as hand written numbers by LeCunn et al\cite{lecun1989}. Since then, CNNs are used in a wide variety of image classification domains\cite{deng2009}. Since CNNs can use images as input to the network, they naturally lend themselves to be applied in this domain as well. Attempts have been made\cite{DBLP:journals/corr/0011S17}, as well as several variants\cite{he2017}\cite{xu2017}\cite{wickpuppe2018}\cite{oliveira2018} of the Fully Convolutional Network (FCN) introduced by Long et al\cite{DBLP:journals/corr/LongSD14}. The major drawback of these techniques is that they do not exploit the often available localized, two-dimensional, textual input obtained through OCR scanning.  

\textbf{Image and text modalities}. The representation of text as input to deep neural networks has increased in complexity since it was first tried in Meier et al\cite{8270006} using a FCN based approach. In this attempt, the textual information was represented as a binary feature (a pixel has text or not), where lexical and semantic dimensions are not taken into account. 

Katti et al\cite{katti2018} introduced the concept of \textit{chargrid}, a two dimensional representation of text where characters are localized on the image and encoded as a one-hot vector. This study shows that model variants exploiting both modalities achieve better results, at the cost of high-computing. While Dang and Nguyen Thanh\cite{DBLP:journals/corr/abs-2106-00952} build on this approach and show an impressive mIoU of 87\% for template-like administrative documents, Denk and Reisswig\cite{DBLP:journals/corr/abs-1909-04948} (who also build upon Katti et al\cite{katti2018}), consider not only characters, but words and their corresponding embeddings. Using BERTgrid for the automatic extraction of key-value information from invoice images (amount, number, date, etc), they obtain the best results with document representation based on one-hot character embedding and word-level BERT embeddings with no image information.

Moving from character, to word, to sentence representations, Yang et al\cite{DBLP:journals/corr/YangYAKKG17} represent text via text embedding maps, where the two-dimensional text representation is mapped to the pixel information. Textual features correspond to sentence embeddings in this case, with the use of word vectors obtained with word2vec\cite{mikolov2013} and averaged in the final embedding. 

Finally, Barman et al\cite{jdmdh:7097} combine word embeddings with a deep neural network approach specifically within the domain of newspaper segmentation. The critical difference to this paper is that Barman et al\cite{jdmdh:7097} uses word embeddings, where we will vary difference sentence embedding encoders. The primary architecture used in Barman et al\cite{jdmdh:7097} is dhSegment, where we will vary different settings within the newer architecture Mask R-CNN, developed specifically for instance segmentation. Similarly to Barman et al\cite{jdmdh:7097}, we will modify the architecture to fit the text embeddings behind the image pixels by extending the number of channels in the image and inserting the text embeddings at the coordinates of the text bounding box produced by the OCR. 

\cleardoublepage
\chapter{Preliminaries}
\label{ch:preliminaries}

In this chapter, we will introduce the theory that our model is based on. Since this thesis only concerns supervised learning, the scope of the preliminaries will only cover supervised learning with neural networks.

\section{Artificial Neural Network}

The term 'Machine Learning' was coined by A. L. Samuel\cite{samuela1959} in the 1950s as he as working on producing what we today know as the minimax algorithm for tabletop games. Similarly, the early building blocks of neural networks were also invented during the same decade by F. Rosenblatt\cite{rosenblatt1958perceptron}. Despite these early discoveries, progress has only in the recent decades taken bounds and leaps beyond classical algorithm performance. Part of the explanation will be examined in this chapter, where we will encounter the difficulties in training these networks, as well as the solutions that modern architectures employ today. The other part of the explanation may be credited to the advances in hardware, enabling complex models to fit in memory and to train in a reasonable timeframe. The time aspect of training can largely be credited to the advancements in the Graphical Processing Unit (GPU).    

\subsection{Perceptron}
The basic building block of an Artificial Neural Network (ANN) is called a perceptron. The perceptron is loosely inspired by the neurons of the brain\cite{rosenblatt1958perceptron}. The arriving signals, called inputs, are multiplied by learnable weights, which is then added to a constant bias value for each input channel. The resulting value from the input, weight and bias is summed up and if the resulting value is above zero, the perceptron outputs a 1. A single perceptron does not terminate if the learning set is not linearly separable. The most famous example of a perceptron's inability to solve problems with linearly nonseparable vectors is the Boolean exclusive-or problem.

\begin{equation}
	f(x) = 
	    \begin{cases}
	      1, & \text{if}\ w * x + b > 0 \\
	      0, & \text{otherwise}
	    \end{cases}
  	\label{eqn:perceptron}
\end{equation}

where $w$ is a vector of real-valued weights, $w * x$ is the dot product \[\sum_{i=1}^{m} w_i x_i\] where $m$ is the number of inputs to the perceptrion, and $b$ is the bias. This perceptron uses a \textit{Heaviside step activation function} whose value is zero for a negative input and one for a positive output.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/perceptron.png}
  \end{center}
  \caption{Model of a perceptron. Bias is omitted in this figure.}
  \label{fig:perceptron}
\end{figure}

To solve more complex problems than binary classification problems in a linearly nonseparable vector space one has to alter the activation function and in many cases use a deeper and wider ensemble of perceptrons. Depending on the activation function used, the perceptron can output a different range of values. Traditionally a very popular activation function was the non-linear \textit{sigmoid activation function}. Lately, due to the problems with vanishing gradients (which we will cover shortly) the piecewise linear function \textit{Rectified Linear Unit} (ReLU) is the standard activation function today.

Perceptrons can be stacked after and next to one another (in the latter case known as a layer), called a multi-layer perceptron (MLP). The first layer, where the input signal is the input data itself, represented in floating point numbers, is called the input layer. The last layer, where the output signal corresponds to the decision or classification, is called the output layer. Any layer between the input layer and the output layer is called a hidden layer. Together these layers form what is known as an Artificial Neural Network. Any ANN with at least two hidden layers is called a Deep Neural Network (DNN).



\section{Training a Deep Neural Network}

Neural networks can learn to approximate functions through learning. Learning occurs by updating the weights and biases of the perceptrons gradually to approximate the desired output value of the last layer in the neural network. This difference between the desired output value and the output of the network, also known as the cost, is estimated by using a cost function. Cross-entropy is a popular cost function. In equation \ref{eqn:crossentropy}, \(y\) is the true label, and \(\hat{y}\) is the predicted value of the current model.

\begin{equation}
	cost(y, \hat{y}) = - \sum\limits_{i=1}^{n} y_i log(\hat{y_i})
  	\label{eqn:crossentropy}
\end{equation}

By updating the weights according to a paradigm, the cost can be minimized. A gradient-based approach to minimizing the cost is called backpropagation\cite{Rumelhart1986}. Backpropagation is defined as taking the following steps:

\begin{enumerate}
\item Forward the input through the neural network to generate a prediction $\hat{y}$.
\item Measure the cross-entropy loss e.g. according to Formula \ref{eqn:crossentropy} 
\item Propagate back the error’s gradient from the last(output) to the first(input) layer of the network.
\item Change the weights and biases based on the gradient.
\end{enumerate}

The error or loss can be calculated according to Formula \ref{eqn:crossentropy} as such: 

\begin{equation}
	J(w_j) = cross\_entropy(y_j,\hat{y_j}) = - \sum_{j} y_j log(\hat{y_j})
  	\label{eqn:jacobian}
\end{equation}


where $w_j$ is the weights of the DNN at layer $j$ and $\hat{y}$ can be seen as a non-linear function of the weights. The updated weight values can be seen in Formula \ref{eqn:weights}. 

\begin{equation}
	w_{i,j}^{(next)} = w_{i,j} - \eta \frac{\partial J(w_j)}{\partial w_i}
  	\label{eqn:weights}
\end{equation}

To update the weight $w_{i,j}$ using gradient descent, a \textit{learning rate} $\eta > 0$ must be chosen. Each value of the updated weights in the next layer depends on the partial derivative of the previous layer. That is why each activation function has to be differentiable (Nevertheless, the ReLU activation function is non-differentiable at 0), and why the error has to be \textit{propagated backwards}. 

\textbf{Should this be explained in more detail? Cases where the $w_i$ is an output neuron vs hidden. }

\section{Challenges in training}

In real world applications, there are many challenges when training a deep neural network.

\subsection{Overfitting/underfitting}

With a large number of parameters, the network can be said to have a high degree of freedom. It becomes so complex that it will encounter the overfitting problem\cite{doi:10.1021/ci0342472}. Overfitting means that the network will tend towards having lower accuracy of the test dataset, despite an increasing accuracy on the training dataset. This phenomenon can be anecdotally compared to memorizing the training dataset by heart, and thus losing the generalization property when tested on data outside the training dataset. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/overfitting.png}
  \end{center}
  \caption{Model complexity and overfitting\cite{Wang_2019}. Test sample prediction error increases as the model overfits on the training sample.}
  \label{fig:overfitting}
\end{figure}

Several approaches can be taken to minimize or resolve the problem of overfitting.
\begin{enumerate}
\item Regularization\cite{10.1038/317314a0}: By imposing a cost (or penalty) on the optimization function, the optimal solution can be made unique.
\item Early stopping\cite{Prechelt2012}: By halting the training when the error of the test dataset reaches its minimum, overfitting can be avoided. Early stopping can be thought of as regularization in the temporal domain. 
\item Dropout\cite{JMLR:v15:srivastava14a}: By randomly ignoring some neurons during training to reduce the model complexity, an increase in model sparsity is gained. 
\item Data augmentation\cite{10.2307/2289457}: Additional data is created, typically in preprocessing, by altering the original data in a number of ways which are added in addition to the original data. For visual data, some examples are: changing angles, illumination, saturation, exposure, etc. This creates additional variance which prevents simply memorizing shallow features in the dataset.
\end{enumerate}

\subsection{Exploding/vanishing gradients}
When the backpropagation algorithm backflows the error gradient from the output layer back towards the input layer, the gradient will diminish by the layer if the initial gradient is smaller than 1.0, and increase substantially when initially greater than 1.0. As a result, some parameters will either remain unchanged by the diminished gradient or changed by a very large value by the time the update process reaches very deep in large networks.

Some techniques for overcoming the exploding/vanishing gradient problem are\cite{doi:10.1142/S0218488598000094}:

\begin{enumerate}
\item Non-saturating activation functions: Using e.g. ReLU\cite{maas2013rectifier} instead of the commonly used sigmoid activation function can help during training. The gradient is minimal when the sigmoid function is close to its output limits $(-1/1)$ meaning it will saturate. Using ReLU = $max(0,z)$ won't have this problem. However, ReLU has its own problem in that some perceptrons may stop outputting values except for 0. This is known as the dying ReLU problem. The solution is to use leaky ReLU = $max(az,z)$ where $a$ is the slope when $z < 0$. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/leaky_relu.png}
  \end{center}
  \caption{Leaky ReLU\cite{Wang_2019}. The leaky ReLU activation function prevents the \textit{dying ReLU problem}.}
  \label{fig:leaky_relu}
\end{figure}

\item Gradient clipping\cite{quintana1974clipping}: Clipping the gradient by defining a max value it can attain during backpropagation means it will never exceed a certain threshold. This solves the exploding gradient problem.

\item Batch Normalization\cite{ioffe2015batch}: A technique used to address the problem that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change, by zero centering the input, scaling and then shifting the result. Due to the impracticality of using the global information for normalization, each normalization occurs at each mini-batch $B$ of size $m$. The mean $\mu_{B}$ and variance $\sigma_B^2$ can be calculated as follows:

\begin{equation}
	\mu_B = \frac{1}{m_B} \sum_{i=1}^{m_B} x^{(i)}
  	\label{eqn:bnmean}
\end{equation}

And thus follows the calculation of the \textit{variance} $\sigma_B^2$ of $B$:

\begin{equation}
	\sigma_B^2 = \frac{1}{m_B} (x^{(i)} - \mu_B)^2
  	\label{eqn:bnvariance}
\end{equation}

For a layer of the network with d-dimensional input, $ x = (x^{(1)}, ..., x^{(d)}) $, each dimension of its input is then normalized (re-centered and re-scaled) separately:

\begin{equation}
	\hat{x}_{i}^{(k)} = \frac{x_{i}^{(k)} - \mu_{B}^{(k)}}{\sqrt{\sigma_{B}^{(k)^2} + \epsilon}}
  	\label{eqn:bnorm}
\end{equation}

where $\epsilon$ is an arbitary small constant added for numerical stability.

\end{enumerate}
\subsection{Improving gradient descent}

Training speed can be improved by using techniques to update the learning rate during training, making the model converge faster towards the desired weight distribution.
\begin{enumerate}
\item Momentum\cite{Rumelhart1986}: In order to improve an otherwise static learning rate, momentum was introduced by Rumelhart, Hinton and Williams. The idea is to remember the update $\Delta w$ at each iteration, and determine the next update as a linear combination of the gradient and the previous update: 

\begin{equation}
	w = w - \eta \nabla Q_i(w)- \alpha \Delta w
  	\label{eqn:momentum}
\end{equation}

where the parameter $w$ which minimizes $Q(w)$ is estimated, $\eta$ is the learning rate, and $\alpha$ is an exponential decay factor between 0 and 1.

\item Weight decay: \textbf{TODO if we use it}
\end{enumerate}


\section{Convolutional Neural Network}

Traditional DNNs had, at the time of creation, very impressive results on many datasets and tasks. However, working with 2-D images, certain problems arose related to the architecture. Traditional MLPs encountered difficulty with learning spatial and translation features, such as lighting or distortion variance. The difficulty in learning spatial features is directly related to how DNNs treat each pixel individually as an input vector and thus losing spatial information on how pixels relate to each other.

\textbf{Convolutional layer}. A 2-D image consists of 3 dimensions: The width, height and channels (Typically 3: Red, green and blue). The convolutional layer, when applying its operation to the input, slides across the image (also referred to as convolves) from the top left corner to the bottom right corner as depicted in Figure \ref{fig:kernelmovement}, performing a matrix multiplication operation between the kernel(a matrix of weights) and the portion of the image it is currently located at. Each kernel operation produces a number(after a bias value is added) which is inserted into a resulting matrix, named a \textit{filter}.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/kernel_movement.png}
  \end{center}
  \caption{Movement of the kernel within the input image or filter matrix\cite{Kang2020DeepCN}.}
  \label{fig:kernelmovement}
\end{figure}


Options for determining the resulting output size of the convolutional layer are called \textbf{stride} and \textbf{zero-padding}. Stride determines how many pixels the kernel shifts as it traverses. Increasing the stride will decrease the number of operations performed, and thus the output size. Since the kernel will pass the border of the image input fewer times than the pixels located closer to the center of the image, zero-padding can be used to improve the performance of the convolutional layer. Zero-padding adds zeros to the edges of the image, making it possible to traverse the original corners and borders of the image multiple times. The logical consequence of artificially increasing the width and height of the image is that more convolves occur, and thus the output size of the layer is increased. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/padding.png}
  \end{center}
  \caption{\textbf{Zero-padding} the image input (in blue and on bottom) by 1 makes the convolutional layer produce the same output dimensions(green and on top) as the input. When the same dimensionality is produced, it is known as \textbf{same-padding}\cite{Kang2020DeepCN}.}
  \label{fig:padding}
\end{figure}

\textbf{Pooling layer.} Similar to how convolutional layers work, pooling layers are responsible for reducing the spatial size of the convolved features. This is to decrease the computational power required to process the data through dimensionality reduction. Another feature of the pooling layers is that, as we reduce the dimensionality of the layers further down the architecture, we force the layers to learn on higher and higher levels of abstraction of the features, since detail is lost with the compression of information. There are two important types of pooling layers: \textbf{Max pooling} and \textbf{average pooling}. As the names suggest, the operation applied to inputs of the layers is the maximum of the values or the average of the values respectively. 

\textbf{Dense layer}. Typically in these architectures, the final layers are traditional dense layers. These layers are used to output the probability of a certain class to be detected within the image. 

\textbf{Residual block}. As networks became deeper to improve results, the problems with vanishing and exploding gradients became more commonplace again. Residual neural network (ResNet), was proposed by Kaiming He et al\cite{he2016deep}. It’s main contribution to the CNN architecture was the residual block, which introduced a short cut layer in order to improve learning as seen in Figure \ref{fig:residual}. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/residual_block.png}
  \end{center}
  \caption{Residual block\cite{he2016deep}.}
  \label{fig:residual}
\end{figure}

\section{Fully Convolutional Network}

Achieving state-of-the-art results on PASCAL VOC\cite{DBLP:journals/corr/LongSD14}, among other datasets, at the time of creation, Fully Convolutional Networks (FCN) output a per-pixel prediction of the segmentation by first convoluting the features and the de-convoluting back to the original size of the input layer as can be seen in Figure \ref{fig:fcn}. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/fcn.png}
  \end{center}
  \caption{Fully Convolutional Network architecture as depicted in the original paper\cite{DBLP:journals/corr/LongSD14}.}
  \label{fig:fcn}
\end{figure}

This is done by removing the final classifier layer and converting all fully connected layers to convolutions. They then append a $ 1 x 1$ convolution with channel dimensions $N$ where $N$ is the number of classes for the specific dataset, followed by a \textit{deconvolution} layer to bilinearly upsample the coarse output to pixel-dense outputs.

The deconvolution layer simply reverses the process of the convolutional layer, explained previously in this chapter.

\section{Feature Pyramid Network}

Recognizing objects at vastly different scales is one of the fundamental challenges in computer vision. One approach which has proven to be successful in both performance on competitive datasets (e.g. the COCO detection benchmark without bells and whistles in this case) and in the sense that it adds little overhead, is the Feature Pyramid Network (FPN). The idea is intuitive: To offset the problem of objects being of vastly different scales in images, the features maps generated from the images are scaled to different resolutions as seen in Figure \ref{fig:fpn}, and predictions are made on each of these scaled feature maps.

Previous approaches similar to the FPN also leverage the pyramid scheme to predict objects at different resolutions. However, as in the case of Featurized image pyramid a) in Figure \ref{fig:fpn}, inference time could increase by up to four times making the approach impractical for real world applications.

Convolutional neural networks typically narrow it’s hidden layers in common architectures, either to make a prediction based on high-level features as in the case of the common heads used in the R-CNN family of networks, or in the case of FCNs, where the layers typically decrease in size only to increase towards the output layers in order to produce a proposal of equal dimensions as the input layer. FPNs exploit the architecture of the CNNs by using the existing hidden layers at different resolutions to predict bounding boxes and classes directly from the hidden layers, reducing the overhead of the FPN backbone, and thus becoming a feasible option for real world applications.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/fpn.png}
  \end{center}
  \caption{Evolution of the Feature Pyramid Network\cite{DBLP:journals/corr/LinDGHHB16}.}
  \label{fig:fpn}
\end{figure}

\chapter{Data Collection}
\label{ch:data}

A broad overview of the different datasets labeled specifically for this paper will be introduced. Then a formal definition of a news unit will be made, which will be derived from the definition of a news article. This definition will be based on what a user might like to query as content on an article-level search of a newspaper. Since the data, made available by Kungliga Biblioteket is strictly non-disclosure (meaning no example figures from the real data can be submitted) an attempt to produce a systematic overview of the contents of the dataset will explain what elements may or may not be included in the definition of a news unit.

\section{Datasets}

Three datasets from three different time periods were produced during this project. The time periods were chosen carefully to differ typographically. The first and largest dataset, DN 2010 - 2020,  is exclusively labeled from the newspaper Dagens Nyheter. Model training is performed on a subset of the DN 2010 - 2020 dataset, which is split into three parts: Train, test and validation. The remaining two datasets are used for evaluation of the trained models.

In each dataset, the labels are produced at the page level. In the first dataset, DN 2010 - 2020, the first newspaper was labeled from start to end. After that, random sampling was used for the remaining data. 


\subsection{DN 2010 - 2020 (Training)}

Sampled from Dagens Nyheter in the time period 2010 to 2020, this dataset contains plenty of articles with rich graphics such as visualizations of data related to the content. Many images related to the articles are very large, sometimes spanning the entire first page of a multi-page article. 

\begin{table}[!ht]
  \begin{center}
    \caption{Class distribution in the DN 2010 - 2020 dataset}
    \label{tab:trainclassdist}
    \begin{tabular}{l|l|l|l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Dataset} & \textbf{News Unit} & \textbf{Advertisement} & \textbf{Listing} & \textbf{Death Notice} & \textbf{Game} & \textbf{Weather}  \\ 
    \hline
    \textbf{Train} & 721 & 427 & 110 & 69 & 13 & 10 \\    \hline
    \textbf{Test} & 394 & 214 & 29 & 7 & 7 & 4 \\    \hline
    \textbf{Validation} & 340 & 177 & 48 & 50 & 6 & 3 \\    \hline
    \end{tabular}
  \end{center}
\end{table}


\subsection{DN SvD 2001 - 2004 (In-set)}

\textbf{Text describing dataset}

\begin{table}[!ht]
  \begin{center}
    \caption{Class distribution in the  DN SvD 2001 - 2004 dataset}
    \label{tab:insetclassdist}
    \begin{tabular}{l|l|l|l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Dataset} & \textbf{News Unit} & \textbf{Advertisement} & \textbf{Listing} & \textbf{Death Notice} & \textbf{Game} & \textbf{Weather}  \\ 
    \hline
    \textbf{Test} & ? & ? & ? & ? & ? & ? \\    \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Aftonbladet Expressen ? - ? (Out-set)}

\textbf{Text describing dataset}

\begin{table}[!ht]
  \begin{center}
    \caption{Class distribution in the  Aftonbladet Expressen ? - ? dataset}
    \label{tab:outsetclassdist}
    \begin{tabular}{l|l|l|l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Dataset} & \textbf{News Unit} & \textbf{Advertisement} & \textbf{Listing} & \textbf{Death Notice} & \textbf{Game} & \textbf{Weather}  \\ 
    \hline
    \textbf{Test} & ? & ? & ? & ? & ? & ? \\    \hline
    \end{tabular}
  \end{center}
\end{table}


\section{Page Layouts}

In this section we will attempt to give a systematic overview of the types of content a newspaper within the dataset contains. 

\subsection{Placards}

The placard, usually found as advertisement for the newspaper itself, typically consists of a single page containing the main story headline and is included in the datasets. 

\subsection{Front pages}

The front page of a newspaper generally contains one or more logos of the newspaper, the date of publication, a “main story” consisting of a headline, a subhead, a body text, and a reference page number. Smaller elements advertising other articles are usually more concise, typically containing no more than a headline and a reference page number. Advertisements occur consistently on front pages although to a lesser degree than in content pages. 

\subsection{Content Pages}

Single page articles are the most common form of content page. In this context, an article fits on a single page, but the page itself may contain many different articles and other elements such as advertisements. If two elements discuss the same topic but have different authors they will be considered two different news units.

Multi page articles are content pages in which the content spans multiple pages. Thus, a single page may only display half of an image, or a few elements of the complete body text. If the page is the first of a multi page article the byline may be missing, but the elements will still count as a news unit. Similarly, if the page is not the first of a multi page article, a headline may be missing, but the elements will still count as a news unit.

\subsection{Listings}

There are many different types of listings that commonly occur in newspapers. These listings are typically dense lists of information, such as stock listings, TV-Radio broadcast timetables, or movie timetables.

\subsection{Comics, artwork and poems}

Comics, artwork and poems pose an interesting challenge in labeling because of the creative freedom of the composition of elements. Many of them border on being included in the definition of a news unit.

Comics, as opposed to news article related images, are typically hand-drawn with optional separator to introduce a narrative, with speech bubbles narrating the fictional characters. Comics received no label in this project.

Artwork is also common in some sections of the newspapers. In this project artwork is treated like a potential image to a news article, as it is often accompanied by a text.

Poems can range from an essay to a single word with the author omitted.

\subsection{Games, puzzles and quizzes}

There are a number of games prevalent in the dataset. The most common being crossword puzzles and sudoku puzzles. Less frequent games that do occur are card game positions.

Quizzes are generally structured as a list of questions. The key to these questions can often be found on the same page in a smaller font, often upside-down.

\subsection{Weather}

Weather information can almost always be found in newspapers. They usually consist of tables of temperatures or weather states, maps of the regions that the newspaper published in, and images of weather states. 


\section{Class Labels}

\subsection{News Unit}

Due to the spatial requirements or stylistic choices made by the publicist, some news article elements may be omitted. The consequence of this fact is that news articles vary greatly in their composition. Thus, when labeling news articles, a systematic definition has to be produced so that labels are consistent across newspapers. In this section, we will attempt to produce such a definition derived from the definition of a news article.

\newtheorem{thm}{Theorem}
\newtheorem{defn}[thm]{Definition}
\begin{defn}
\label{newsarticle}
A \textbf{news article} is a written work published in a print or electronic medium with the purpose of propagating news, research results, academic analysis or debate. News articles tend to be composed of a headline, a subhead, a preamble, a body text, and a byline. Some elements, such as the subhead and preamble, may be omitted due to stylistic choices or space requirements. News articles are often extended by a number of other optional elements, such as images and image captions. Digitally published news articles may also include video and video captions.
\end{defn}

\begin{defn}
\label{headline}
A \textbf{headline} is a very short summary of a news report. It normally appears in large letters above the report. [Cambridge Dictionary]
\end{defn}

\begin{defn}
\label{subhead}
A \textbf{subhead} or subheading is a word, phrase, or sentence that is used to introduce part of a text. [Cambridge Dictionary]
\end{defn}

\begin{defn}
\label{preamble}
A \textbf{preamble} is an introduction to a speech or piece of writing. [Cambridge Dictionary]
\end{defn}

\begin{defn}
\label{body text}
A \textbf{body text} is the main part of a printed text, excluding items such as headings and footnotes. [Oxford Languages]
\end{defn}

\begin{defn}
\label{byline}
A \textbf{byline} is a line at the top or bottom of a newspaper or magazine article giving the writer's name. [Cambridge Dictionary]
\end{defn}

\begin{defn}
\label{img}
An \textbf{image caption} or a \textbf{video caption} is a title or brief explanation accompanying an illustration, cartoon, poster or in the latter case a video. [Oxford Languages]
\end{defn}

Given Defintion \ref{newsarticle} of a \textit{news article}, we will now introduce the concept of a \textit{news unit}, which will determine what elements will be labeled in the newspaper data as representative of a news article.

\begin{defn}
\label{newsunit}
A \textbf{news unit} is a written work published in a print or electronic medium with the purpose of propagating news, research results, academic analysis or debate. It’s minimum requirement is that it contains at least:

\begin{enumerate}
\item a headline or a preamble or a body text or an image with an optional image caption
\item a reference page number or a byline
\end{enumerate}
\end{defn}

A \textit{news unit} may contain all of the above, including an image of the author themselves and smaller elements of text that usually contain explanatory contents, such as definitions or facts associated with the article itself. The label bounding box will be the smallest spanning bounding box, including white spaces between the elements, should they make up a rectangular shape. In the case that elements are organized such that they do not make up a rectangular shape, a smallest rectangular shape will be made excluding the protruding elements, which are then included by extending the border of the original rectangular shape in an asymmetric manner.

\textbf{TODO explain with images}

\subsection{News Unit Delimitations}

Some elements were included in the news unit due to the fact that they contained text relevant to the article itself. These elements are:

\begin{enumerate}
\item Facts \& figures relating the the content
\item References to previous articles on the same topic
\item Corrections to previous articles on the same topic
\end{enumerate}

\subsection{Advertisement}

The most common form of advertisement found in our dataset is that of rectangular images possibly containing text. They vary greatly in size, sometimes stretching the entire page. Purely textual advertisement also occurs frequently, sometimes with clear bounding boxes to indicate a separation from the journalistic content. Advertisements usually contain contact information of the provider of a service or product. 

There are advertisements for the newspaper itself. It could be an offer for a subscription to the newspaper, an encouragement from the newspaper company to participate in a debate, an ad for the ad space itself, or a way to give feedback to the authors. Elements that suggest to the reader to visit the website of the newspaper or to explore other sections of the newspaper are labeled as Advertisement. Elements that are list-like instructions to participate remain unlabeled due to their generic format and repeated textual content.

Finally one must mention that there are advertisements made to look like news articles. They are intentionally structured and formatted to appear exactly as news articles do, except for the fact that a small warning is made that the section is in fact an advertisement. In this project, these types of advertisements are labeled as news articles. \textbf{TODO maybe discuss more?}

\subsection{Listing}

Listings are typically very distinguishable from the other types of content, yet there are some examples where the label could be more difficult to establish. In the case of movie advertisement, depending on the style of presentation the format could be argued to be similar to a Listing.

Yet in this project, all movie related content is labeled as Advertisement. Another form of content that can easily border between Listing and Advertisement is the “Kungörelser” section(English: Announcement or notification). Sometimes containing advertisements, the format is usually very dense and a brief language is used. “Kungörelser” are labeled as Listings, although their individual subheadings remain unlabeled.

\subsection{Weather}

Weather information is labeled as the Weather class. Sometimes appearing as whole-page content, sometimes as snippets or previews on other pages, the smallest bounding box containing all types of weather information (maps, tables, graphics) are labeled as an instance. In the case that weather information is split by other types of content, two instances are labeled.

\subsection{Death Notice}

Death notices are obvious in their pattern. A strong black border surrounds a text describing the person, usually beginning with a smaller graphic, the name of the person and an addition of information regarding the person or details regarding their passing. On pages usually containing death notices, In memoriam texts occur: Longer texts describing the person in question, usually submitted by a number of people. These submissions are labeled as news units as per the definition. 

\subsection{Game}

Any form of game found within the newspaper is labeled as one and the same class: Game.  

\subsection{Publication Unit}

In order to investigate the impact of classes on the performance of the model, we perform the same experiments where all annotations are set to the same class. In the experiments using the publication unit, all other classes are converted to the same class. Therefore, if an element or group of elements are assigned any class, previously mentioned in this chapter, they will be labeled as a publication unit in the experiments where class confusion is investigated.

\subsection{Delimitations}

Titles, section titles and subsection titles are not included in the definition of any class. The guiding principle is that content labeled as a class should be presenting detailed news. Plenty of elements found in the newspaper are simply repeated and do not, by themselves, convey any news. Some of the elements act as a guide for the reader to navigate the newspaper, e.g. topics or sections. Other elements convey information about the newspaper itself, usually repeated in every edition found in the datasets. A comprehensive list of these unlabeled elements follows:

\begin{description}
\item[$\bullet$] Titles
\item[$\bullet$] Logos of newspaper
\item[$\bullet$] Section titles
\item[$\bullet$] Subsection titles
\item[$\bullet$] Instructions how to participate in debates
\item[$\bullet$] Footers and headers describing the news team / publishing team
\end{description}


\cleardoublepage

\chapter{Model Architecture}
\label{ch:modelarch}

Our objective is to segment newspaper images and to classify detected segments of news units according to a fine-grained newspaper section typology. To this end, we introduce a method which performs supervised, pixel-wise multiclass classification using both visual and textual features. This method builds on the Mask R-CNN architecture by He et al\cite{DBLP:journals/corr/HeGDG17}. While varying the primary architecture, we explore the use of sentence transformers that map sentences of text to a vector space, which is embedded in the image input channels.

\section{Primary Architecture}

Mask R-CNN is a deep learning neural network architecture designed for object instance detection and segmentation. Given an input image, the network produces bounding boxes, segmentation masks, and an estimated class for each object detected in the image. Mask R-CNN is an extension in a longer line of extensions of neural network architectures.

\textbf{R-CNN}. The reason the original CNN is not ideal to use for object detection in images is that the output may be variable since you might have a variable amount of objects being detected in the image, making the output size vary from one image to the next. Region Based Convolutional Neural Network (R-CNN), proposed by Ross Girshick et al\cite{Girshick_2014_CVPR}, uses a method where selective search is used to extract 2000 regions from the image called region proposals. Each proposal is warped into a square and fed into a CNN that produces a 4096-dimensional feature vector as output. In this case, the CNN acts as a feature extractor and the output dense layer consists of the features extracted from the image. These features are fed into a Support Vector Machine to classify the presence of an object within the region proposal. The algorithm also predicts four values which are offset values to increase the precision of the bounding box.  

\textbf{Fast R-CNN}. The next evolution of the R-CNN, called “Fast R-CNN”\cite{Girshick_2015_ICCV}, hints at the problems that R-CNN had and that its extension is trying to solve. R-CNN took a very long time to produce its output, and the selective search algorithm used to propose the regions is fixed, neglecting the possibility to learn in this stage of the architecture. Fast R-CNN introduces a RoI (Region of Interest) pooling layer, making it possible to skip generating 2000 region proposals and instead only have the convolution operation done once per image. 

\textbf{Faster R-CNN}. Fast, but not fast enough, the Fast R-CNN still took a significant time generating the region proposal, albeit much faster than its predecessor. Shaoqing Ren et al\cite{7485869} came up with a replacement for the selective search algorithm with the added bonus that it lets the network learn in the region proposal step. This part of the network is called the Region Proposal Network (RPN). 

\textbf{Mask R-CNN}. Mask R-CNN contributes to this architecture by adding a parallel prediction of masks alongside its original output which is a class label and a bounding-box offset. This parallel proposal is what made Mask R-CNN unique compared to other extensions of Faster R-CNN at the time, where other methods often depended on the mask predictions for the class prediction. Mask R-CNN also introduces the concept of RoIAlign, to address misalignment issues between the RoI and the extracted features which impacts the pixel-accurate masks it produces due to quantization which is avoided with RoIAlign. More thorough details can be found in the original Mask R-CNN paper, He et al\cite{DBLP:journals/corr/HeGDG17}. All deviations from the standard configuration will be noted in the implementation details chapter of this paper.

\textbf{Network modularity}. The Mask R-CNN architecture is made up of modular networks, which can be varied. The modules are:

\begin{description}
\item[$\bullet$ Backbone] Used for feature extraction of an entire image
\item[$\bullet$ Head] Used for bounding-box recognition (classification and regression)
\end{description}

\section{Text Embedding map}

In order to combine the visual and textual information as input to the model, we map the one-dimensional representation of textual information (e.g. a word vector or paragraph vector) into a three-dimensional representation by positioning the embedding representation into a two-dimensional space (e.g. a word or paragraph has a certain width, height, x-, and y-coordinate of the top left corner when printed on a page). 

This new textual embedding vector, corresponding to a character, word or paragraph, is equivalent to the original vector, augmented with the positional information. We refer to this three-dimensional representation of textual information, as mentioned in Barman et al\cite{jdmdh:7097} and introduced in Yang et al.\cite{DBLP:journals/corr/YangYAKKG17} as a ‘text embedding map’.

Notably, the key difference from the earlier works by Barman et al\cite{jdmdh:7097} and Yang et al\cite{DBLP:journals/corr/YangYAKKG17} is that in this approach the text embedding maps are generated on a paragraph level, instead of character- or word level.

The three-dimensional encoding of textual information is produced by using the result of an OCR process which outputs coordinates of bounding boxes for blocks of text together with its textual content.

We define this process formally as follows. Given an image of size $H$ $x$ $W$ and a list of tokens $T$ where each token $t$ is associated with a bounding box $b_t$ on the image, a text embedding map $G$ of size $H$ $x$ $W$ $x$ $N$ is produced, where $N$ is the dimension of the embeddings. Specifically, all pixels contained in the bounding box of a token $t$ are defined as the set $b_t \in {\rm I\!R}^2$ and each pixel $g_{i,j} \in G$ of the text embedding map is computed with 

\begin{equation}
	g_{ij} = 
	    \begin{cases}
	      E(t), & \text{if}\ (i,j) \in b_t \\
	      0^N, & \text{otherwise}
	    \end{cases}
  	\label{eqn:textembedding}
\end{equation}

where $E(t)$ is a mapping of $t \rightarrow {\rm I\!R}^N$ corresponding for example to a paragraph embedding, and $0^N$ is a null vector in case there is no text in the corresponding pixel. Each pixel overlapping with a bounding box produced by the OCR process is therefore mapped to its corresponding embedding. In this project, no bounding boxes are overlapping, which means no statement will be made regarding a strategy for such a case.

The result is a text embedding map, i.e. a three-dimensional matrix where the first two dimensions correspond to the image-localized representation of the text, and the third to the embedding itself. Given that the resulting matrix has the same width and height as the original image, it lends itself to easily extend the image input layer to accept more than its 3 original channels (RGB), that is to also fit an N-dimensional vector behind each pixel in a preprocessing step. The final number of channels of the input layer is therefore $3+N$, where the first 3 dimensions of each pixel contain the original RGB values of the input image. 

\textbf{TODO Maybe visualization of the embedding maps?}

\subsection{Modifications to the primary architecture}

\textbf{Preprocessing}. Text embeddings are added to the data during the preprocessing step. Because we want to avoid image normalization of the textual embeddings, and other preprocessing methods used by Mask R-CNN to augment the image data, the textual embeddings are added as the last step. By the time we add the textual embeddings, the image may have been resized, padded and flipped horizontally. The bounding boxes of the text embeddings are offset and scaled accordingly to align with the new dimensions.

\textbf{Evaluation}. In the original COCO dataset (on which Mask R-CNN is usually evaluated), there is size partitioning depending on the size of the object being detected when presented in the results. Since the classes found in this project are generally larger on average than the size of the objects found in the coco dataset, the partitioning size thresholds are adjusted for these results to provide more useful information about different objects being detected(Using the standard thresholds, almost all objects are considered “large”). This alteration has no impact on the performance of the solution itself, but must be considered when evaluating the results.

\begin{table}[!ht]
  \begin{center}
    \caption{Object size threshold differences}
    \label{tab:objectsizes}
    \begin{tabular}{l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Dataset} & \textbf{Small} & \textbf{Medium} & \textbf{Large}  \\ 
    \hline
    COCO dataset & area < $32^2$ & $32^2$ < area < $96^2$ & area > $96^2$  \\    \hline
    This project & area < X & X < area < X & area > X \\    \hline
    \end{tabular}
  \end{center}
\end{table}


\chapter{Evaluation}
\label{ch:eval}

\section{Metrics}

\subsection{Mean Intersection over Union}

The Intersection over Union (IoU) is the standard metrics for semantic image segmentation and measures how well two sets of pixels are aligned. This is used to measure how much the boundary predicted by the algorithm overlaps with the ground truth (the real object boundary). Traditionally, with state-of-the-art datasets, an IoU threshold equal or greater to 0.5 \textbf{(? TODO verify)} is used to classify whether the prediction is a true positive or a false positive.  

\begin{equation}
	IoU = Area Overlap / Area Union
  	\label{eqn:iou}
\end{equation}


\subsection{Precision and Recall}

The IoU does not qualify performances in terms of True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN). These values are relevant when considering whether a model can be useful for real world applications, that is, if most of the segments are correctly recognized. 

The standard way to measure these values in the field of segmentation is to consider a prediction as positive when above a certain threshold $\tau \in [0,1]$ of IoU. If the prediction is above the threshold $\tau$, it is well enough aligned with the ground truth and to be considered correct. 

Given this definition, we can now consider:
\begin{enumerate}
\item A prediction with IoU $\geq \tau$ as True Positive(TP)
\item A prediction with no IoU (i.e. with a union of zero) as True Negative(TN)
\item A prediction with an IoU of zero and no predicted pixels (i.e. intersection of zero and non-zero number of pixels in the ground truth) as False Negative(FN)
\item A non-FN prediction with an IoU $< \tau$ as False Positive(FP)
\end{enumerate}

Given a threshold $\tau$, precision and recall are computed as follows:

\begin{equation}
	Precision @ \tau = P@\tau = \frac{TP}{(TP + FP)}
  	\label{eqn:precision}
\end{equation}

\begin{equation}
	Recall @ \tau = R@\tau = \frac{TP}{(TP + FN)}
  	\label{eqn:recall}
\end{equation}

It is also possible to compute the average precision and recall over a range of thresholds. This is defined by a start $\tau_{start}$, an end $\tau_{end}$, and a step size between two threshold $\tau_{step}$ using the following notation: $\tau_{start}$:$\tau_{step}$:$\tau_{end}$. Given a threshold range, the average metric $M$ (Where $M$ could be for example precision or recall) is then computed as follows:

\begin{equation}
M@\tau_{start}:\tau_{step}:\tau_{end} = \frac{1}{(\tau_{start}:\tau_{step}:\tau_{end})} \sum{} \tau \in \tau_{start}:\tau_{step}:\tau_{end}
    \label{eqn:rangem}
\end{equation}

It should be noted that metrics such as IoU, mIoU, Precision and Recall are computed at the page level and not at the instance level. If a page contains several instances of a class and the prediction only matches some instances, thus not enough to reach an IoU threshold larger than $\tau$, the whole page is counted as negative. (TODO verify!)


\subsection{Mean Average Precision}

The mean Average Precision (mAP) was used to evaluate detection performance. The mAP metric is the product of precision and recall of detected bounding boxes. The mAP value ranges from 0 to 1, with higher values being better. The mAP can be computed by calculating average precision (AP) separately for each class (including background) and then averaging over the classes. A detection is considered a true positive only if the IoU is above a certain threshold. 

All detections from test images may be combined in a precision/recall curve. The final area under the curve is used to compare the algorithms. If $N$ represents the number of images, the mAP can be computed as follows:

\begin{equation}
mAP = \frac{1}{N} \sum_{i=1}^{N} AP_i
    \label{eqn:map}
\end{equation}

\section{Major Results}

In this section we present the results of the neural network architectures for the datasets created in this thesis. Naming convention follows the original Mask R-CNN paper and results are presented by bounding box ($bb$) and segmentation mask ($m$) respectively, as it is presented in the Mask R-CNN paper.

\begin{table}[!htbp]
  \begin{center}
    \caption{Model names}
    \label{tab:modelnames}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|l|l|p{1.5cm}} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Final name} & \textbf{Head} & \textbf{Backbone} & \textbf{Sentence transformer} & \textbf{Text embeddings length} \\ 
    \hline
    resnet50-FPN-vanilla & resnet50 & FPN & None & 0 \\    \hline
    resnext101-32x8d-FPN-vanilla & resnext101 & FPN & None & 0 \\    \hline
    resnext101-64x4d-FPN-vanilla & resnext101 & FPN & None & 0 \\     \hline
    More & models & in & final & paper \\     \hline
    resnet50-FPN-KBLab & resnext101 & FPN & KBLab/sentence-bert-swedish-cased & 768 \\     \hline
    resnet50-FPN-KB & resnet50 & FPN & KB/bert-base-swedish-cased & 768 \\     \hline
    \end{tabular}}
  \end{center}
\end{table}

\begin{table}[!htbp]
  \begin{center}
    \caption{Results on the DN 2010 - 2020 (Training) dataset}
    \label{tab:dn2010results}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|l|l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Final name} & \textbf{$mAP_m$} & \textbf{$mAP_m^{50}$} & \textbf{$mAP_m^{75}$} & \textbf{$mAP_{bb}$} & \textbf{$mAP_{bb}^{50}$} & \textbf{$mAP_{bb}^{75}$} \\ 
    \hline
    resnet50-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\    \hline
    resnext101-32x8d-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\    \hline
    resnext101-64x4d-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\     \hline
    \end{tabular}}
  \end{center}
\end{table}

\begin{table}[!htbp]
  \begin{center}
    \caption{Results on the DN SvD 2001 - 2004 (In-set) dataset}
    \label{tab:dnsvd2001results}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|l|l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Final name} & \textbf{$mAP_m$} & \textbf{$mAP_m^{50}$} & \textbf{$mAP_m^{75}$} & \textbf{$mAP_{bb}$} & \textbf{$mAP_{bb}^{50}$} & \textbf{$mAP_{bb}^{75}$} \\ 
    \hline
    resnet50-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\    \hline
    resnext101-32x8d-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\    \hline
    resnext101-64x4d-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\     \hline
    \end{tabular}}
  \end{center}
\end{table}
\begin{table}[!htbp]
  \begin{center}
    \caption{Results on the Aftonbladet Expressen ? - ? (Out-set) dataset}
    \label{tab:abexpresults}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|l|l|l|l|l} % <-- Alignments: 1st column left, 2nd middle, with vertical lines in between
    \textbf{Final name} & \textbf{$mAP_m$} & \textbf{$mAP_m^{50}$} & \textbf{$mAP_m^{75}$} & \textbf{$mAP_{bb}$} & \textbf{$mAP_{bb}^{50}$} & \textbf{$mAP_{bb}^{75}$} \\ 
    \hline
    resnet50-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\    \hline
    resnext101-32x8d-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\    \hline
    resnext101-64x4d-FPN-vanilla & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\     \hline
    \end{tabular}}
  \end{center}
\end{table}

\chapter{Discussion}
\label{ch:disc}

\chapter{Conclusions and Future Work}
\section{Conclusions}
\section{Limitations}
\section{Future work}

%The thesis contributes to the \gls{UN}\enspace\glspl{SDG} numbers 1 and 9 by
%xxxx. 

\noindent\rule{\textwidth}{0.4mm}

\cleardoublepage
% Print the bibliography (and make it appear in the table of contents)
\renewcommand{\bibname}{References}
\addcontentsline{toc}{chapter}{References}

\ifbiblatex
    %\typeout{Biblatex current language is \currentlang}
    \printbibliography[heading=bibintoc]
\else
    \bibliography{references}
\fi




\cleardoublepage
\appendix
\renewcommand{\chaptermark}[1]{\markboth{Appendix \thechapter\relax:\thinspace\relax#1}{}}


\textcolor{black}{black} {\color{black} \rule{\linewidth}{1mm} }

%% The following label is necessary for computing the last page number of the body of the report to include in the "For DIVA" information
\label{pg:lastPageofMainmatter}

\clearpage
\fancyhead{}  % Do not use header on this extra page or pages
% TODO diva stuff
%\section*{For DIVA}
%\lstset{numbers=none} %% remove any list line numbering
%\divainfo{pg:lastPageofPreface}{pg:lastPageofMainmatter}
\end{document}
